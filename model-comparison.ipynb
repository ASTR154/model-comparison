{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "By: Griffin Hosseinzadeh (2025 May 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by generating some synthetic data from a slightly more complicated model than usual. The true underlying model will be a straight line plus a sinusoid. Imagine, for example, that these data are measurements of the brightness of a variable star over time. It could have short-timescale oscillations in brightness while also increasing in average brightness over the long term. Let's model this as\n",
    "$$ y_3(x) = m x + b + a\\sin(2\\pi x) $$\n",
    "where the three model parameters are the slope $m$, the intercept $b$, and the amplitude of the oscillations $a$.\n",
    "We're going to compare this model with a simpler model without the oscillations:\n",
    "$$ y_2(x) = m x + b $$\n",
    "When we have noisy data, we may not be able to tell the difference between these two models.\n",
    "\n",
    "First, write both these models down as functions. We will want to call them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3(x, m, b, a):\n",
    "    return   # complete\n",
    "\n",
    "def model2(x, m, b):\n",
    "    return   # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 100 random points in the range $0<x<1$ and evaulate the true model ($y_3$) at these points given the parameters below. Then add Gaussian noise to each point using the value of $\\sigma$ below. Plot your synthetic data set (with error bars) and visually compare it with your two proposed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100       # number of points\n",
    "m_true = 10.  # slope\n",
    "b_true = 50.  # intercept\n",
    "a_true = 1.   # amplitude\n",
    "sigma = 1.    # scatter\n",
    "\n",
    "rng = np.random.default_rng(seed=13579)\n",
    "x = rng.uniform(  # random values between 0 and 1\n",
    "y_noise_free = model3(  # complete\n",
    "y =  # complete\n",
    "dy =  # complete\n",
    "\n",
    "x_model =  # complete\n",
    "y_model2 =  # complete\n",
    "y_model3 =  # complete\n",
    "\n",
    "# plot\n",
    "# plot\n",
    "# plot\n",
    "# plot\n",
    "# plot\n",
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we are pretending we don't know which model is true, or what the true values of the parameters are. We have a data set, and we want to figure out which model is a a better fit, and whether it is significantly better or not, given the different numbers of parameters.\n",
    "\n",
    "For each model, write down functions for the logarithm of the prior, likelihood, and posterior. **Make sure your prior is properly normalized.** Use these functions to calculate the Bayesian evidence for each model. This is very similar to what we did in the Bayesian statistics exercise, with one additional step: numerical integration (`np.trapezoid`) of the entire posterior grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_min = 0.5  # technically cannot be =0 for a log-uniform prior\n",
    "m_max = 99.5\n",
    "b_min = 0.5\n",
    "b_max = 99.5\n",
    "\n",
    "\n",
    "def log_prior2(theta):\n",
    "    # complete\n",
    "\n",
    "def log_likelihood2(theta, x, y, dy):\n",
    "    # complete\n",
    "\n",
    "def log_posterior2(theta, x, y, dy):\n",
    "    # complete\n",
    "\n",
    "\n",
    "m_range2 =  # complete\n",
    "b_range2 =  # complete\n",
    "theta2 =  # complete\n",
    "posterior_grid2 =  # complete\n",
    "\n",
    "evidence2 =  # complete\n",
    "print(evidence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_min = 0.1\n",
    "a_max = 10.\n",
    "\n",
    "\n",
    "def log_prior3(theta):\n",
    "    # complete\n",
    "\n",
    "def log_likelihood3(theta, x, y, dy):\n",
    "    # complete\n",
    "\n",
    "def log_posterior2(theta, x, y, dy):\n",
    "    # complete\n",
    "\n",
    "\n",
    "m_range3 =  # complete\n",
    "b_range3 =  # complete\n",
    "theta3 =  # complete\n",
    "posterior_grid3 =  # complete\n",
    "\n",
    "evidence3 =  # complete\n",
    "print(evidence3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the odds ratio for the 3-parameter model compared to the 2-parameter model. What does this number tell you? Does it agree with your intuition from the plot you made above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Complete*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the Akaike and Bayesian information criteria (AIC & BIC). For the sake of time, do not worry about determining the actual maximum-likelihood parameter values. Just use the true parameter values we used to synthesize the data at the beginning of this notebook. (Presumably those should be close to the maximum-likelihood values.) What do the AIC and BIC tell you? Do they agree with the odds ratio and/or your intuition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "aic2 =  # complete\n",
    "aic3 =  # complete\n",
    "print(aic2, aic3, aic2 - aic3)\n",
    "\n",
    "bic2 =  # complete\n",
    "bic3 =  # complete\n",
    "print(bic2, bic3, bic2 - bic3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Complete*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat this entire notebook using a different data set with increased noise. Either duplicate this entire notebook (File > Duplicate), in which case you need to add that new notebook to the GitHub repository, or copy all the code cells above and paste them below. *(Yes, you could just change the number and rerun the notebook, but we want to see a record of both.)*\n",
    "\n",
    "This time, increase $\\sigma$ to 5. Comment on your results, specifically whether or not you can distinguish between the two models, and whether you think that is reasonable or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Complete*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
